{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n"
     ]
    }
   ],
   "source": [
    "print('starting...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 基于GAN方法云层消除"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils import data\n",
    "from attrdict import AttrMap\n",
    "from torch import optim\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from eval import test\n",
    "from models.circle_gan.gen.gen import Generator\n",
    "from models.circle_gan.dis.circle_dis import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open('config.yml', 'r', encoding='UTF-8') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config = AttrMap(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据增强"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(config.img_size, config.img_size),\n",
    "    ])\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(config.img_size, config.img_size),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集定义"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class TrainDataset(data.Dataset):\n",
    "    # def train_list_init(self):\n",
    "    #     files = os.listdir(os.path.join(config.datasets_dir, 'ground_truth'))\n",
    "    #     random.shuffle(files)\n",
    "    #     n_train = int(config.train_size * len(files))\n",
    "    #     train_list = files[:n_train]\n",
    "    #     test_list = files[n_train:]\n",
    "    #     np.savetxt(os.path.join(config.datasets_dir, config.train_list), np.array(train_list), fmt='%s')\n",
    "    #     np.savetxt(os.path.join(config.datasets_dir, config.test_list), np.array(test_list), fmt='%s')\n",
    "\n",
    "    def __init__(self, config, img_list, transforms):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transforms = transforms\n",
    "        # 如果数据集尚未分割，则进行训练集和测试集的分割\n",
    "        # if not os.path.exists(train_list_file) or os.path.getsize(train_list_file) == 0:\n",
    "        # train_list_init()\n",
    "\n",
    "        self.img_list = img_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t = cv2.imread(os.path.join(config.datasets_dir, 'ground_truth', str(img_list[index])), 1).astype(\n",
    "            np.float32)\n",
    "        x = cv2.imread(os.path.join(config.datasets_dir, 'cloudy_image', str(img_list[index])), 1).astype(\n",
    "            np.float32)\n",
    "        if self.transforms is not None:\n",
    "            x = self.transforms(image=x)['image']\n",
    "            t = self.transforms(image=t)['image']\n",
    "\n",
    "        M = np.clip((t - x).sum(axis=2), 0, 1).astype(np.float32)\n",
    "        x = x / 255\n",
    "        t = t / 255\n",
    "        x = x.transpose(2, 0, 1)\n",
    "        t = t.transpose(2, 0, 1)\n",
    "\n",
    "        return x, t, M\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(img_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 工具函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  42\n",
      "===> Loading datasets\n",
      "train dataset: 400\n",
      "validation dataset: 400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed_manage(config)\n",
    "print('===> Loading datasets')\n",
    "train_list_file = os.path.join(config.datasets_dir, config.train_list)\n",
    "assert (len(train_list_file) > 0)\n",
    "\n",
    "img_list = np.loadtxt(train_list_file, str)\n",
    "train_img_list, valid_img_list = train_test_split(img_list, random_state=42, test_size=config.validation_size)\n",
    "train_dataset = TrainDataset(config, transforms=train_transform, img_list=train_img_list)\n",
    "valid_dataset = TrainDataset(config, transforms=val_transform, img_list=valid_img_list)\n",
    "\n",
    "print('train dataset:', len(train_dataset))\n",
    "print('validation dataset:', len(valid_dataset))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "training_data_loader = DataLoader(dataset=train_dataset, num_workers=config.threads, batch_size=config.batchsize,\n",
    "                                  shuffle=True)\n",
    "validation_data_loader = DataLoader(dataset=valid_dataset, num_workers=config.threads,\n",
    "                                    batch_size=config.validation_batchsize, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "gen = Generator(gpu_ids=config.gpu_ids)\n",
    "\n",
    "if config.gen_init is not None:\n",
    "    param = torch.load(config.gen_init)\n",
    "    gen.load_state_dict(param)\n",
    "    print('load {} as pretrained model'.format(config.gen_init))\n",
    "\n",
    "dis = Discriminator(in_ch=config.in_ch, out_ch=config.out_ch, gpu_ids=config.gpu_ids)\n",
    "\n",
    "if config.dis_init is not None:\n",
    "    param = torch.load(config.dis_init)\n",
    "    dis.load_state_dict(param)\n",
    "    print('load {} as pretrained model'.format(config.dis_init))\n",
    "\n",
    "# setup optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=config.lr, betas=(config.beta1, 0.999), weight_decay=0.00001)\n",
    "opt_dis = optim.Adam(dis.parameters(), lr=config.lr, betas=(config.beta1, 0.999), weight_decay=0.00001)\n",
    "\n",
    "real_a = torch.FloatTensor(config.batchsize, config.in_ch, config.img_size, config.img_size)\n",
    "real_b = torch.FloatTensor(config.batchsize, config.out_ch, config.img_size, config.img_size)\n",
    "M = torch.FloatTensor(config.batchsize, config.img_size, config.img_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 损失函数定义"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from log_report import LogReport\n",
    "from log_report import TestReport\n",
    "\n",
    "criterionL1 = nn.L1Loss()\n",
    "criterionMSE = nn.MSELoss()\n",
    "criterionSoftplus = nn.Softplus()\n",
    "\n",
    "if config.cuda:\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    criterionL1 = criterionL1.cuda()\n",
    "    criterionMSE = criterionMSE.cuda()\n",
    "    criterionSoftplus = criterionSoftplus.cuda()\n",
    "    real_a = real_a.cuda()\n",
    "    real_b = real_b.cuda()\n",
    "    M = M.cuda()\n",
    "\n",
    "real_a = Variable(real_a)\n",
    "real_b = Variable(real_b)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job number: 0014\n"
     ]
    }
   ],
   "source": [
    "make_manager()\n",
    "n_job = job_increment()\n",
    "\n",
    "print('Job number: {:04d}'.format(n_job))\n",
    "\n",
    "config.out_dir = os.path.join(config.out_dir, '{:06}'.format(n_job))\n",
    "os.makedirs(config.out_dir)\n",
    "\n",
    "logreport = LogReport(log_dir=config.out_dir)\n",
    "validationreport = TestReport(log_dir=config.out_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('===> begin')\n",
    "    start_time = time.time()\n",
    "    # main\n",
    "    for epoch in range(1, config.epoch + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        for iteration, batch in enumerate(training_data_loader, 1):\n",
    "            real_a_cpu, real_b_cpu, M_cpu = batch[0], batch[1], batch[2]\n",
    "            real_a.resize_(real_a_cpu.size()).copy_(real_a_cpu)\n",
    "            real_b.resize_(real_b_cpu.size()).copy_(real_b_cpu)\n",
    "            M.resize_(M_cpu.size()).copy_(M_cpu)\n",
    "            if config.use_attention:\n",
    "                att, fake_b = gen.forward(real_a)\n",
    "            else:\n",
    "                fake_b = gen.forward(real_a)\n",
    "            ################\n",
    "            ### Update D ###\n",
    "            ################\n",
    "\n",
    "            opt_dis.zero_grad()\n",
    "\n",
    "            # train with fake\n",
    "            fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "            pred_fake = dis.forward(fake_ab.detach())\n",
    "            batchsize, _, w, h = pred_fake.size()\n",
    "\n",
    "            loss_d_fake = torch.sum(criterionSoftplus(pred_fake)) / batchsize / w / h\n",
    "\n",
    "            # train with real\n",
    "            real_ab = torch.cat((real_a, real_b), 1)\n",
    "            pred_real = dis.forward(real_ab)\n",
    "            loss_d_real = torch.sum(criterionSoftplus(-pred_real)) / batchsize / w / h\n",
    "\n",
    "            # Combined loss\n",
    "            loss_d = loss_d_fake + loss_d_real\n",
    "\n",
    "            loss_d.backward()\n",
    "\n",
    "            if epoch % config.minimax == 0:\n",
    "                opt_dis.step()\n",
    "\n",
    "            ################\n",
    "            ### Update G ###\n",
    "            ################\n",
    "\n",
    "            opt_gen.zero_grad()\n",
    "\n",
    "            # First, G(A) should fake the discriminator\n",
    "            fake_ab = torch.cat((real_a, fake_b), 1)\n",
    "            pred_fake = dis.forward(fake_ab)\n",
    "            loss_g_gan = torch.sum(criterionSoftplus(-pred_fake)) / batchsize / w / h\n",
    "\n",
    "            # Second, G(A) = B\n",
    "            loss_g_l1 = criterionL1(fake_b, real_b) * config.lamb\n",
    "            if config.use_attention:\n",
    "                loss_g_att = criterionMSE(att[:, 0, :, :], M)\n",
    "                loss_g = loss_g_gan + loss_g_l1 + loss_g_att\n",
    "            else:\n",
    "                loss_g = loss_g_gan + loss_g_l1\n",
    "\n",
    "            loss_g.backward()\n",
    "\n",
    "            opt_gen.step()\n",
    "\n",
    "            # log\n",
    "            if iteration % 10 == 0:\n",
    "                print(\n",
    "                    \"===> Epoch[{}]({}/{}): loss_d_fake: {:.4f} loss_d_real: {:.4f} loss_g_gan: {:.4f} loss_g_l1: {:.4f}\".format(\n",
    "                        epoch, iteration, len(training_data_loader), loss_d_fake.item(), loss_d_real.item(),\n",
    "                        loss_g_gan.item(), loss_g_l1.item()))\n",
    "\n",
    "                log = {}\n",
    "                log['epoch'] = epoch\n",
    "                log['iteration'] = len(training_data_loader) * (epoch - 1) + iteration\n",
    "                log['gen/loss'] = loss_g.item()\n",
    "                log['dis/loss'] = loss_d.item()\n",
    "\n",
    "                logreport(log)\n",
    "\n",
    "        print('epoch', epoch, 'finished, use time', time.time() - epoch_start_time)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_validation = test(config, validation_data_loader, gen, criterionMSE, epoch)\n",
    "            validationreport(log_validation)\n",
    "        print('validation finished')\n",
    "        if epoch % config.snapshot_interval == 0:\n",
    "            checkpoint(config, epoch, gen, dis)\n",
    "\n",
    "        logreport.save_lossgraph()\n",
    "        validationreport.save_lossgraph()\n",
    "        print('training time:', time.time() - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def _lr_decay_step(G_lr_decay, D_A_lr_decay, D_B_lr_decay, current_iteration):\n",
    "    G_lr_decay.step(current_iteration)\n",
    "    D_A_lr_decay.step(current_iteration)\n",
    "    D_B_lr_decay.step(current_iteration)\n",
    "\n",
    "\n",
    "def train_cycle():\n",
    "    device = torch.device('cuda')\n",
    "    from circlegan.models import Generator, Discriminator\n",
    "    criterion_GAN = torch.nn.MSELoss().cuda()\n",
    "    criterion_cycle = torch.nn.L1Loss().cuda()\n",
    "    criterion_identity = torch.nn.L1Loss().cuda()\n",
    "\n",
    "    G_A2B = Generator(64, 9)\n",
    "    D_B = Discriminator(config.img_size, 64, 4)\n",
    "    G_A2B.apply(weights_init_normal)\n",
    "    D_B.apply(weights_init_normal)\n",
    "    G_A2B = torch.nn.DataParallel(G_A2B).to(device)\n",
    "    D_B = torch.nn.DataParallel(D_B).to(device)\n",
    "\n",
    "    G_B2A = Generator(64, 9)\n",
    "    D_A = Discriminator(config.img_size, 64, 4)\n",
    "    G_B2A.apply(weights_init_normal)\n",
    "    D_A.apply(weights_init_normal)\n",
    "    G_B2A = torch.nn.DataParallel(G_B2A).to(device)\n",
    "    D_A = torch.nn.DataParallel(D_A).to(device)\n",
    "\n",
    "    G_optimizer = optim.Adam(itertools.chain(G_B2A.parameters(), G_A2B.parameters()), lr=0.0002,\n",
    "                             betas=[0.5, 0.999])\n",
    "    D_A_optimizer = optim.Adam(D_A.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "    D_B_optimizer = optim.Adam(D_B.parameters(), lr=0.0002, betas=[0.5, 0.999])\n",
    "\n",
    "    G_lr_decay = optim.lr_scheduler.StepLR(G_optimizer, step_size=50000, gamma=0.1)\n",
    "    D_A_lr_decay = optim.lr_scheduler.StepLR(D_A_optimizer, step_size=50000, gamma=0.1)\n",
    "    D_B_lr_decay = optim.lr_scheduler.StepLR(D_B_optimizer, step_size=50000, gamma=0.1)\n",
    "\n",
    "    ones = None\n",
    "    zeros = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, config.epoch + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        for iteration, batch in enumerate(training_data_loader, 1):\n",
    "            print('iteration:{}'.format(iteration))\n",
    "            real_a_cpu, real_b_cpu, _ = batch[0], batch[1], batch[2]\n",
    "            real_a.resize_(real_a_cpu.size()).copy_(real_a_cpu)\n",
    "            real_b.resize_(real_b_cpu.size()).copy_(real_b_cpu)\n",
    "\n",
    "            if ones is None and zeros is None:\n",
    "                ones = torch.ones_like(D_A(real_a))\n",
    "                zeros = torch.zeros_like(D_A(real_b))\n",
    "\n",
    "            _lr_decay_step(G_lr_decay, D_A_lr_decay, D_B_lr_decay, iteration)\n",
    "\n",
    "            #########################################################################################################\n",
    "            #                                                     Generator                                         #\n",
    "            #########################################################################################################\n",
    "            fake_a = G_B2A(real_b)\n",
    "            fake_b = G_A2B(real_a)\n",
    "\n",
    "            # gan loss\n",
    "            gan_loss_a = criterion_GAN(D_A(fake_a), ones)\n",
    "            gan_loss_b = criterion_GAN(D_B(fake_b), ones)\n",
    "            gan_loss = (gan_loss_a + gan_loss_b) / 2.0\n",
    "\n",
    "            # cycle loss\n",
    "            cycle_loss_a = criterion_cycle(G_B2A(fake_b), real_a)\n",
    "            cycle_loss_b = criterion_cycle(G_A2B(fake_a), real_b)\n",
    "            cycle_loss = (cycle_loss_a + cycle_loss_b) / 2.0\n",
    "\n",
    "            # idnetity loss\n",
    "            identity_loss_a = criterion_identity(G_B2A(real_a), real_a)\n",
    "            identity_loss_b = criterion_identity(G_A2B(real_b), real_b)\n",
    "            identity_loss = (identity_loss_a + identity_loss_b) / 2.0\n",
    "\n",
    "            # overall loss and optimize\n",
    "            g_loss = gan_loss + 10 * cycle_loss + 5 * identity_loss\n",
    "\n",
    "            loss_g = g_loss\n",
    "\n",
    "            G_optimizer.zero_grad()\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            G_optimizer.step()\n",
    "\n",
    "            #########################################################################################################\n",
    "            #                                                     Discriminator                                     #\n",
    "            #########################################################################################################\n",
    "            # discriminator a\n",
    "            gan_loss_a_real = criterion_GAN(D_A(real_a), ones)\n",
    "            gan_loss_a_fake = criterion_GAN(D_A(fake_a.detach()), zeros)\n",
    "            gan_loss_a = (gan_loss_a_real + gan_loss_a_fake) / 2.0\n",
    "\n",
    "            D_A_optimizer.zero_grad()\n",
    "            gan_loss_a.backward()\n",
    "            D_A_optimizer.step()\n",
    "\n",
    "            # discriminator b\n",
    "            gan_loss_b_real = criterion_GAN(D_B(real_b), ones)\n",
    "            gan_loss_b_fake = criterion_GAN(D_B(fake_b.detach()), zeros)\n",
    "            gan_loss_b = (gan_loss_b_real + gan_loss_b_fake) / 2.0\n",
    "\n",
    "            D_B_optimizer.zero_grad()\n",
    "            gan_loss_b.backward()\n",
    "            D_B_optimizer.step()\n",
    "\n",
    "            loss_d = gan_loss_b + gan_loss_a\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                print(\n",
    "                    \"===> Epoch[{}]({}/{}): gen_loss: {:.4f} dis_loss: {:.4f}\".format(\n",
    "                        epoch, iteration, len(training_data_loader), loss_g.item(), loss_d.item()))\n",
    "                log = {}\n",
    "                log['epoch'] = epoch\n",
    "                log['iteration'] = len(training_data_loader) * (epoch - 1) + iteration\n",
    "                log['gen/loss'] = loss_g.item()\n",
    "                log['dis/loss'] = loss_d.item()\n",
    "\n",
    "                logreport(log)\n",
    "\n",
    "\n",
    "        print('epoch', epoch, 'finished, use time', time.time() - epoch_start_time)\n",
    "        with torch.no_grad():\n",
    "            # G_A2B(real_a)\n",
    "            log_validation = test(config, validation_data_loader, G_A2B, criterionMSE, epoch)\n",
    "            validationreport(log_validation)\n",
    "        print('validation finished')\n",
    "\n",
    "        if epoch % config.snapshot_interval == 0:\n",
    "            checkpoint(config, epoch, gen, dis)\n",
    "\n",
    "        logreport.save_lossgraph()\n",
    "        validationreport.save_lossgraph()\n",
    "        print('training time:', time.time() - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练过程"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "C:\\Users\\admin\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Avg. MSE: 0.4846\n",
      "===> Avg. PSNR: 3.4307 dB\n",
      "===> Avg. SSIM: 0.0302 dB\n",
      "iteration:2\n",
      "===> Avg. MSE: 0.3874\n",
      "===> Avg. PSNR: 4.4027 dB\n",
      "===> Avg. SSIM: 0.0430 dB\n",
      "iteration:3\n",
      "===> Avg. MSE: 0.3199\n",
      "===> Avg. PSNR: 5.2144 dB\n",
      "===> Avg. SSIM: 0.0508 dB\n",
      "iteration:4\n",
      "===> Avg. MSE: 0.2622\n",
      "===> Avg. PSNR: 5.9608 dB\n",
      "===> Avg. SSIM: 0.0642 dB\n",
      "iteration:5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16792/2933081501.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mshutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopyfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'config.yml'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mout_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'config.yml'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mtrain_cycle\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16792/2925393949.py\u001B[0m in \u001B[0;36mtrain_cycle\u001B[1;34m()\u001B[0m\n\u001B[0;32m    128\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m                 \u001B[1;31m# G_A2B(real_a)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 130\u001B[1;33m                 \u001B[0mlog_validation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_data_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mG_A2B\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterionMSE\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    131\u001B[0m                 \u001B[0mvalidationreport\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlog_validation\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\DataspellProjects\\CloudRemoval\\eval.py\u001B[0m in \u001B[0;36mtest\u001B[1;34m(config, test_data_loader, gen, criterionMSE, epoch)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mavg_psnr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0mavg_ssim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_data_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m         \u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mVariable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mVariable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 435\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    437\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    473\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    474\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 475\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    476\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    477\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16792/2875434704.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     23\u001B[0m             np.float32)\n\u001B[0;32m     24\u001B[0m         x = cv2.imread(os.path.join(config.datasets_dir, 'cloudy_image', str(img_list[index])), 1).astype(\n\u001B[1;32m---> 25\u001B[1;33m             np.float32)\n\u001B[0m\u001B[0;32m     26\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransforms\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'image'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 保存本次训练时的配置\n",
    "shutil.copyfile('config.yml', os.path.join(config.out_dir, 'config.yml'))\n",
    "\n",
    "train_cycle()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}